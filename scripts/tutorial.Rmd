---
title: "Coding predictors for regression analysis"
subtitle: "An applied tutorial on how to prepare predictors for LMs, GLMs, and GLMMs, how to interpret results depending on these steps, and how to report results"
author: "T. Florian Jaeger"
date: \today
geometry: margin=2cm
header-includes:
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \usepackage{tabto}
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{placeins}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
  - \setstcolor{red}
  - \usepackage{sectsty}
  - \sectionfont{\color{blue}} 
  - \subsectionfont{\color{blue}}
  - \subsubsectionfont{\color{darkgray}}
output:
  pdf_document: 
    fig_caption: yes
    fig_width: 7
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  fontsize: 10pt
---

```{r set-options, include=F}
library(knitr)
opts_chunk$set(dev = 'pdf',
               comment="", 
               echo=FALSE, warning=TRUE, message=TRUE,
               cache=FALSE, 
               size="footnotesize",
               tidy.opts = list(width.cutoff = 250),
               fig.width = 8, fig.height = 4.5, fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

```{r libraries, include=FALSE}
library(tidyverse)
library(magrittr)
library(broom)
```

```{r constants, include=F}
chains = 4

options(
  width = 1000,
  mc.cores = min(chains, parallel::detectCores()))
```

```{r functions}
```


# Reading and assignments in *preparation* of this class

Please make sure you have read *all* of Gelman \& Hill (2007, Ch 3 up to and incl. 4.5) as well as James et al. (2013, Ch 3.3 up to but *not* incl. 3.3.3). Read and *work through* this document. Write up your answers to all the questions. If there are a few questions, you cannot answer, elicit help on the slack channel. 



# Overview
STATE THE PURPOSE OF THIS DOCUMENT.




# Quick recap: The linear model (LM)

A linear model (or linear regression) describes an outcome $y$ as the weighted sum of predictors $x_1$, ..., $x_k$, plus some error $\epsilon$ that is assumed to be normally distributed. This can be written in a variety of ways, e.g., for the specific $i$th of 1 to $n$ outcomes, $y_i$, or for the entire $n$-element vector of outcomes ${\mathbf y}$ with the elements $y_1$, ..., $y_n$, or in terms of the expected value of $E(\mathbf{y})$:

\begin{align}
y_i & = & \beta_0+\beta_1x_{1,i}+...+\beta_kx_{k,i} + \epsilon_i, & \ \epsilon_i \sim \mathrm{Normal}(0, \sigma_{resid}) & \Leftrightarrow \\
{\mathbf y} & = &  \beta_0+\beta_1{\mathbf x_1}+...+\beta_k{\mathbf x_k} + \epsilon, & \ \epsilon_i \sim \mathrm{Normal}(0, \sigma_{resid})  & \Leftrightarrow & \ \mathrm{[cf.\ James\ et\ al., 2013, p.\ 63]}\\
{\mathbf y} & = & X\beta + \epsilon, & \ \epsilon_i \sim \mathrm{Normal}(0, \sigma_{resid})  & \Leftrightarrow & \\
E({\mathbf y}) & = & X\beta &  & \Leftrightarrow & \\
\mathbf{y} & = & \mathrm{Normal}(X\beta, \sigma_{resid})&  &  & \ \mathrm{[cf.\ Gelman\ \&\ Hill, 2007, p.\ 38]}
\end{align}

where $X$ is a matrix with $k + 1$ columns and $n$ rows (the first column of which is a vector of $n$ 1s). $X$ is also sometimes called the *model (or design) matrix*.


## Assumptions of the LM

As a theoretical model the LM makes several assumptions (cf. Gelman \& Hill, 2007, p. 45). The first two assumptions are shared with many/most other statistical models we use (to be precise, some models don't strictly make these assumptions, but they do correct for their violations):

 + Independence of observations/errors
 + Validity/exhaustivity
 
The other assumptions are more specific to the LM, though the last two of these are shared with GLMs and GLMMs (but not GAMMs, for example):

 + Normality of errors
 + Equal variance of errors
 + Additivity of effects 
 + Linearity of each effects
 
Many, if not most, of these assumptions are actually violated when we apply the model. To some extent linear regression is robust to such violations, depending on the specific assumption. For example, instead of equality of variance it is often sufficient that variances are similar (homogeneity of variance) or that the variance-covariance matrix does not exhibit heteroscedasticity (from Ancient Greek hetero "different" and skedasis "dispersion", https://en.wikipedia.org/wiki/Heteroscedasticity; this extends the notion of homogeneity of variance to the *co*variance between variables). **But violations of the independence, normality, or homoscedasticity assumptions can make a model invalid, and the statistical inferences based on it invalid.**

## Using an LM (fit) for your analysis

The linear model is a theoretical model. If we employ it during data analysis, we are making the assumption that this model describes the relation between the predictors and the outcome in the population we seek to study. This also means that all of our conclusions are contingent on these assumptions, including in particular the assumption that we have considered all relevant variables (or have otherwise ruled out that other effects can confound our analysis).

The coefficients of the LM (the $\beta$s) are parameters of this theoretical model (the coefficients), and we do not know their true value. Neither do we know the true values of the outcome, but we assume that our observations of the outcome variable are (potentially noisy) independent samples drawn from the outcome. Additionally, we often don't really know the predictors we believe to have causal effects on the outcome, but rather we have observable *measures* of these predictors. The linear model does *not* take this into account (but there are models that extend the LM that do).

When we use an LM to analyze our data, we fit a *specific* LM---i.e., an LM with a specific set of predictors to our data (the specific outcome). We specify this model using regression formula syntax. For example in R, the following would describe the formula for an LM that regresses an outcome variable Threshold against the intercept (1) and a predictor called Condition:

$$ Threshold \sim 1 + Condition $$
The algorithm implemented in whatever function we use for that (e.g., *fitlm* in Matlab or *lm* in R) then determines the best-fitting estimates of the *coefficients*. In other words, we tell the LM-fitting function the outcome and predictor---the knowns---and the function then determines the coefficients---the unknowns. The resulting combination of the specific LM (the predictors and outcome variables) and the best-fitting coefficient estimates together constitute the **fit** or **fitted (linear) model**. The statistical inferences we draw, and write-up, are always based on such best-fitting models. Here we won't go into detail about how these "best-fitting" estimates and their standard errors are obtained, but we will return to that later in the semester.

## Writing-up results

We should **use $\widehat{\beta}$ when we describe coefficient estimates and write up results**. This emphasizes that these are *estimates* of the assumed population parameters ($\beta$). It is worth noting that the $\widehat{\beta}$s are not the only outputs we obtain from a LM fit. This fit also contains, for example, predicted values for each outcome based on the predictors for that observation. But the null hypothesis significance testing (NHST) that we conduct focuses on the best-fitting coefficient estimates and their standard errors. When we write that a predictor did or did not have a statistically significant effect, or when we talk about the size of the effect, those statements refer to the coefficient estimate, its standard error, and measures derived from them.


# The data for this document

```{r load data, message=F}
d = read_csv("../data/data_ClarkCrowding.csv") %>%
  mutate_at(c("Subject", "Condition"), factor)
d
```



# Coding categorical (and other) predictors

If we want to include categorical predictors (e.g., "Condition A" vs. "Condition B") in an LM, we thus need to translate these predictors into numerical variables. This process is sometimes called predictor *coding*. Some people use this term narrowly to only refer to the coding of categorical predictors (or, as they are sometimes called, *factors*; hence the name factor coding). Others use it to refer to any changes or transformation we make to our predictors, including continuous predictors (e.g. centering or scaling). In this tutorial, we talk about both.

If you have been using analysis of variance (ANOVA), coding is a process that you might already be familiar with: in ANOVA-based result reporting, we regularly follow the initial ANOVA significance tests with so called "planned" or "post-hoc" comparisons. These comparisons assess hypotheses about the means of the different experimental conditions. 

You might have heard terms like (forward/backward) Helmert coding, (forward/backward) sliding difference coding, polynomial coding, sum/deviation/ANOVA coding, or treatment/dummy coding, etc. These are all names for different ways to code categorical factors, and they do exactly the same job in an LM as they do for the planned or unplanned post-hoc analyses after an ANOVA. In fact, the post-hoc tests reported for ANOVA-based approaches are typically run as an LM, but that's another topic. (Note that these post-hoc tests are the only way to obtain information about the *direction* of an effect in the ANOVA approach, since the ANOVA itself only assesses the significance of a predictor). 

In an LM, we need to make these coding decision *prior* to fitting the model. **Most statistics programs have default coding choices built-in that they apply to any LM or other regression you fit. Be aware of these defaults, as they will affect what the output of an LM means if you forgot to explicitly define the factor coding for the LM.** In R, for example, all factors for which we have not explicitly defined coding will be treatment coded, with the reference level set to the first level of the factor (we'll get to what all of this means). And, if you haven't defined how the levels of the factor order, R will assume that they order alphanumerically. 

Such defaults can be handy in that they mean you can just run a model like: 

$$ Threshold \sim 1 + Condition $$

where Threshold is the outcome, $\sim$ refers to "regress against", 1 refers to the intercept (which by default would be included anyway, but I'm being explicit here), and Condition is a factor with two levels "crowded" and "uncrowded". But defaults can also wreak havoc. In this example, R would automatically code "crowded" as the reference level (0), because it comes alphanumerically first, and "non-crowded" as the treatment (1). The coefficient for Condition would thus---perhaps counter-intuitively---reflect the increase in the threshold in the *un*crowded condition relative to the crowded condition. In short:

 + **don't rely on default**; 
 + **code your factors explicitly**;
 + **use variable names and level names that are transparent and avoid confusion**


# Coding a binary categorical predictor
 
Let's start with the simplest case: coding a binary (two-level) factor. This will allow us to build intuitions both about *how* we can code variables, and *what the consequences of those codes are*. We'll stick with the example from above. 

Factor coding essentially translates the categorical predictor into a numerical predictor. This is achieved through so called *contrasts*. Specifically, for a factor with $k$ distinct levels we can use $k-1$ contrasts to create $k-1$ numerical predictors. These numerical predictors capture all the information of the $k$ levels of the original factor. Each contrast is a vector with $k$ numeric elements, describing the mapping from factor levels to numerical values.

For a binary factor, we thus have one contrast that is a vector with two values. This contrast defines one numerical predictor. E.g., by default in R, we'd get the following for Ashley's data set on visual crowding:

```{r}
contrasts(d$Condition)
```

And, each time a regression function is called, this allows R to *implicilty* create a new column in our data that is the numerical translation of the Condition variable. Thus a model like

$$ Threshold \sim 1 + Condition $$

is actually running the model

$$ Threshold \sim 1 + ConditionUncrowded $$

where $ConditionUncrowded$ is a numerical variable that has been silently added to our data:

```{r}
d %>%
  mutate(ConditionUncrowded = ifelse(Condition == "Uncrowded", 1, 0))
```

## Treatment/dummy-coding

The default in many statistics programs is treatment-coding, like in the example we just went through. Under this coding scheme, we contrast all other condition against a baseline (or "reference") condition. Since the default is in this case somewhat counter-intuitive---it's more intuitive to think of visual *crowding* as the treatment---we set define our own treatment coding:

```{r}
contrasts(d$Condition) = cbind("Crowded" = c(1,0))
contrasts(d$Condition)
```

Now we can fit the LM and summarize that *fit*:

```{r}
l.treatment = lm(Threshold ~ 1 + Condition, data = d)
summary(l.treatment)
```

In this treatment-coded model:

 + the intercept estimate give us the model's prediction for the mean outcome *of the reference level* of Condition (because the intercept *always* gives us the model's prediction when all other terms of the model are 0---e.g., when all other predictors are 0). In this case, this is the mean threshold for the uncrowded condition.
 + the ConditionCrowded estimate tells us how much larger (or smaller) the threshold is for the treatment condition. In this case, this is the crowded condition. 
 + the sum of the intercept and the estimate for the coefficient for ConditionCrowded give us the model's prediction for the mean outcome of the treatment level.
 
We can convince ourselves that the above interpretation is correct by looking at the predictions of the model for each observation, and comparing them to the estimates for the intercept and the coefficient for ConditionCrowded. First, let's have a look at the model's predictions, and attach. It's just a vector with as many elements as there are outcomes in the data we fit with the linear regression.

```{r}
fitted(l.treatment)
```

Note that the model only predicts two different values---one for the crowded condition and one for the uncrowded condition. This becomes apparent when we attach the fitted/predicted values to the data.frame. Here I'm only showing the Subject, Condition, Threshold, and fitted values:

```{r}
d %<>%
  mutate(fitted = fitted(l.treatment))
d %>% 
  select(Subject, Condition, Threshold, fitted)
```

For the uncrowded condition, the fitted value is 1.525 ... the exact same as the intercept estimate. For the crowded condition, the fitted value is 2.101, which is 1.525 (the intercept estimate) + .576 (the estimate for the coefficient of ConditionCrowded). Noice!

That is, our LM predicts the following means (black points) for the two conditions, compared to the data that the model was fit to:

```{r, echo=F}
d %>%
  ggplot(aes(x = Condition)) +
  geom_point(aes(y = Threshold, color = Subject), alpha = .5) +
  geom_line(aes(y = Threshold, color = Subject, group = Subject), alpha = .3) +
  stat_summary(aes(y = fitted),
               fun = mean, geom = "point", size = 2) +
  theme_bw()
```


### Questions

 1. How would you calculate the epsilon/residual for each observation from this data?
 1. How could you calculate the residual sum of squares for this model from this data? 
 1. Why does the model predict only two values?
 1. We obtain the straightforward interpretation of the ConditionCrowded estimate only because we coded the treatment condition as 1, rather than, e.g., as 2. What would happen if we coded the treatment condition as 2? Would our estimate for the coefficient for ConditionCrowded change? What about the estimate of the intercept? Why?
 1. Would the predicted/fitted values of the model change if we change the coding?




## Deviation/sum/anova-coding

While treatment-coding is the default in the regression world, it is actually rarely used in experimental psychology, brain imaging, or related fields that use factorial/balanced designs. Rather, we typically code our data in a different way, because it changes the interpretation of the intercept in ways that researchers that are used to ANOVA tend to find more intuitive. This alternative is called deviation/sum/anova-coding:

```{r}
contrasts(d$Condition) = cbind("Crowded.vs.Uncrowded" = c(.5,-.5))
contrasts(d$Condition)
```

Let's refit the model with this newly coded factor:

```{r}
l.deviation = lm(Threshold ~ 1 + Condition, data = d)
summary(l.deviation)
```

In this deviation-coded model:

 + the intercept estimate is now the predicted *overall mean* of the outcome.
 + the estimate for the coefficient of Condition is the predicted difference between the two conditions. 
 + the two conditions means are described as the sum of the intercept estimate +/- .5-times the estimate for the coefficient of Condition.

Why do researchers used to ANOVA find this more intuitive? You'll sometimes here that it's nice that the intercept now corresponds to the mean, but the true convenience of this coding scheme will become apparent once we consider interactions below. It is under deviation-coding, that we can talk about *main effects* and *interactions* in the same sense as in an ANOVA. That's presumably also why some people call this coding scheme anova-coding. We'll get to that. But first some questions.

### Questions

 1. What happens if we double the values we assign to each of the two deviation-coded conditions? What if we set them to -1000 vs. 1000? What changes and what doesn't? Why?

```{r, echo = T}
contrasts(d$Condition) = cbind("Crowded vs. Uncrowded" = c(1,-1))
```

 2. Are the predictions of any of these deviation-coded models different from each other?
 3. Are the predictions of any of these deviation-coded models different from the treatment-coded model presented in the previous section? Why?
 4. We obtain the intuitive interpretation of the intercept (as the predicted grand mean of the data) only because the data is ... what? (recall that the intercept is *always* the predicted value for the case when all other terms add up to zero). Conveniently, most data sets obtained from psychological experiments have this property---either exactly or at least approximately (after exclusions).

## Writing up the analysis results

How would we write up, for example, the deviation-coded model. There are, of course, about as many difference preferences as they are researchers, and so you will find conflicting advice. But generally, it will be helpful if you are clear about all of the following:

 + What model you used
 + What predictors you considered (incl. those that you did not include in the final model, cf. debate about *researchers' degrees of freedom*)
 + How you coded the outcome (e.g., what unit is the outcome in; without this information, readers cannot determine the size of effects or interpret them on the original scale)
 + How you coded predictors (without this information, readers cannot even determine in which *direction* the effect is!)
 + What steps (if any) were taken to ascertain the validity of the model (see, e.g., model evaluation in Gelman \& Hill, 2007, Section 3.7). This is arguably less important if you study a well-known phenomenon that has been analyzed with this method many times, and for which you have data that is a) balanced with regard to the predictors in the model, and b) has many observations relative to the number of predictors in the model.

When reporting results, I recommend you provide both the relevant statistics (coefficient estimates, $p$-values, etc.), and a descriptive interpretation of that result in non-technical language---but we careful to avoid language that is wrong (see, e.g., section on interactions for some examples). Finally, for any non-trivial model, I highly recommend the use of a summary table of the model and visualization of the *empirical* distribution of the data (potentially, while *also* showing the model's predictions). The latter helps readers not familiar with the analysis approach to understand your results.



### An example write-up for the deviation-coded model

Here is a rather detailed write-up for our one-predictor model. In many scenarios (e.g., if the units of the outcome variable are not necessarily informative), you might well provide less detail:

\color{lightgray}
We analyzed the data with a linear regression, using the function \texttt{lm} from the \texttt{base} package (citation with version) of the software \texttt{R} (citation with version). We calculated each subject's mean thresholds for the crowded and uncrowded condition. These 20 threshold values were regressed against condition (deviation-coded: .5 = *crowded* vs. -.5 = *uncrowded*). We found a statistically significant main effect of condition ($\widehat{\beta}=.576, t=3.657, p<.01$), so that subjects reached threshold performance at spacing that was about half an arc minute larger in the crowded condition (mean = `r  reduce(tidy(l.deviation)[["estimate"]], .f = function(x, y) round(x + .5 * y, 3))`), compared to the uncrowded condition (mean = `r  reduce(tidy(l.deviation)[["estimate"]], .f = function(x, y) round(x - .5 * y, 3))`).
\color{black}









# Combining factors and continuous predictors

Now that we know how to code factors (or at least binary factor), we can combine continuous and categorical predictors in our model. We first show a simple 'additive' model, in which we assume that the effects of the categorical and continuous predictors are additive. Then we consider a model that also contains an interaction, allowing the two effects to be more or less than additive. **The models presented here merely serve the purpose of illustrating how we can fit, analyze, and interpret models with continuous and categorical predictors. The one degree of freedom we have in our model so far (Condition) already puts as at the maximum of the recommended degrees of freedom for 20 data points.** Including additional parameters, as we do below, increases the risk of overfitting the model to the data.


## Does the diffusion constant affect threshold performance beyond the experimental condition?

For example, let's test the effects of both Condition and DiffusionConstant. For simplicity's sake, we continue to use deviation coding for Condition:

$$ Threshold \sim 1 + Condition + DiffusionConstant $$
What happens when we include both of these predictors in the LM?

```{r}
l.combined = lm(Threshold ~ 1 + Condition + DiffusionConstant, data = d)
summary(l.combined)
```

Right away, we can see that the diffusion constant seems to account for a *lot* of additional variability in the model: the $R^2$ of the model is almost twice as large as the one we obtained when only considering condition. The output of the regression also tell us that both of the predictors have statistically significant effects on subjects' threshold performance.  

### Write-up

Here's a somewhat less detailed write-up for our two-predictor model, building on the example provided above:

\color{lightgray}
We analyzed the data with a linear regression, using the function \texttt{lm} from the \texttt{base} package (citation with version) of the software \texttt{R} (citation with version). We calculated each subject's mean thresholds for the crowded and uncrowded condition. These 20 threshold values were regressed against condition (deviation-coded: .5 = *crowded* vs. -.5 = *uncrowded*) and the diffusion constant. We found a statistically significant effect of condition ($\widehat{\beta}=.431, t=4.738, p<.01$), so that subjects reach threshold performance at larger spacings in the crowded condition, compared to the uncrowded condition. We also found a statistically significant effect of the diffusion constant ($\widehat{\beta}=.035, t=6.373, p<.01$), so that larger diffusion constants required larger spacing to achieve threshold performance.
\color{black}

### Questions

 1. Notice how that the intercept estimate in this model is not the same as in the model that only contains Condition as a predictor. That also means that the intercept no longer represents the prediction for the grand mean of the outcome. Was that inevitable? Can you think of a scenario in which the inclusion of the additional predictor (DiffusionConstant) would *not* change the intercept estimate? (Recall that the intercept always is the model's prediction when all other terms of the model add to 0.)
 1. The coefficient estimate for Condition also has changed. Specifically, it is now somewhat smaller (.431 vs. .576). What do you make out this? Does it tell you something about the relation between the two *predictors* (Condition and DiffusionConstant)?
 1. What would happen if you centered the predictor DiffusionConstant (by subtracting its mean from each of its values): how would the above model output change? Does the slope of either of the two predictors change? Does the value of the intercept change? Do the predicted $\widehat{y}$s of the model change?
 1. What is an intuitive geometric interpretation of this model? The effect of a single continuous predictor (DiffusionConstant) is described by the intercept and slope. So what does the present model result in? (If you get stuck in thinking about this, reread Gelman \& Hill, 2007, p. 31-33).
 1. Can we conclude from this model, and the fact that the two predictors are both significant, that the two effects are additive?
 1. Can we conclude that DiffusionConstant has an effect in both crowdedness conditions?




# Coding of continuous predictors
common transformations that can make the model more interpretable (though sometimes at other costs).

## Centering
re-establishes interpretation of intercept as grand mean, and meaning of main effects.

```{r}
d %<>%
  mutate(across(where(is.numeric), list("c" = function(x) x - mean(x))))

summary(lm(Threshold ~ 1 + Condition + DiffusionConstant_c, data = d))
```

## Scaling 
effect size comparison

## Scaling by 2 standard deviations. Why?

## Non-linear transformations







# Interactions between factors and continuous predictors

Next, we expand the analysis further. We remove the additivity assumption. Or rather, the addidivity assumption still holds but we expand the model in a way that we are not assuming the our two predictors are additive. This is done by including an interaction between the two predictors. Here I show this for the case of one continuous predictor (DiffusionConstant) and one binary categorical predictor (Condition), but the same logic extends to interactions between multiple continuous or multiple categorical predictors, as well as interactions between interactions (e.g., three-way interactions, etc.). For the case of one continuous and one categorical predictor, the geometric interpretation of the model is that we now allow the two lines (corresponding to the continuous predictor's effect at the two levels of the categorical predictor) to not be parallel. And our question of whether the interaction is *significant* becomes the question whether the difference in the slope of the two lines is statistically different from zero.

We can ask this question by adding a new predictor to the model that is the *product* of the two predictors. We then ask whether this new predictor has a non-zero effect, i.e., we ask whether the coefficient for this new predictor is different from zero. In R, the regression formula for this model is written as (where the colon is the interaction operator):

$$ Threshold \sim 1 + Condition + DiffusionConstant + Condition:DiffusionConstant$$
which essentially runs the following model (where I is the identity operator, which return x for x):

$$ Threshold \sim 1 + Condition + DiffusionConstant + I(Condition * DiffusionConstant)$$

or shorter (where X1 * X2 is a shorthand for the full-factorial combination of X1 and X2):

$$ Threshold \sim 1 + Condition*DiffusionConstant$$
Just like, e.g., factor coding implicit creates an additional numerical variable that encodes the factor's information, the interaction operator implicitly creates a new variable in our data that is the product if the (numerically coded) factor Condition and the continuous predictor DiffusionConstant.


## Do the effects of condition and difficusion constant interact?

Now we can fit the new LM with the interaction, and look at the summary of results:

```{r}
l.interaction = lm(Threshold ~ 1 + Condition * DiffusionConstant, data = d)
summary(l.interaction)
```


l.interaction = lm(Threshold ~ 1 + Condition * DiffusionConstant_c, data = d)
summary(l.interaction)


### Questions 

 1. What's a geometric interpretation of an interaction between two *continuous* predictors (x1 and x2)? For this it might be helpful to first think about the geometric interpretation of two additive continuous predictors (a plane over x1 and x2, the height of which is given along the third axis, y)



common mis-summaries of the results (for which simple tests are required)




## Simple effects




## Writing up the analysis results

What does this mean?





## centered

To analyze whether 

```{r}
l.interaction = lm(Threshold ~ 1 + Condition * DiffusionConstant_c, data = d)
summary(l.interaction)
```

# Factors with more than two levels

## Helmert coding

## Sliding difference coding

## Polynomial coding

## Interactions
What's shared with binary case, what's not?


# Final considerations for continuous predictors

## Non-linear transformation

polynomials
log

Why is this still a *linear* model? What does the linearity assumption really mean? (linearity in coefficients)


# Session info
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
