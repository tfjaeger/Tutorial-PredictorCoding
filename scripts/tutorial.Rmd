---
title: "Coding predictors for regression analysis"
subtitle: "An applied tutorial on how to prepare predictors for LMs, GLMs, and GLMMs, how to interpret results depending on these steps, and how to report results"
author: "T. Florian Jaeger"
date: \today
geometry: margin=2cm
header-includes:
  - \usepackage{booktabs}
  - \usepackage{siunitx}
  - \usepackage{tabto}
  - \usepackage{soul}
  - \usepackage{xcolor}
  - \usepackage{placeins}
  - \usepackage{lscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
  - \makeatletter\renewcommand{\fps@table}{!ht}\makeatother
  - \setstcolor{red}
  - \usepackage{sectsty}
  - \sectionfont{\color{blue}} 
  - \subsectionfont{\color{blue}}
  - \subsubsectionfont{\color{darkgray}}
output:
  pdf_document: 
    fig_caption: yes
    fig_width: 7
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  fontsize: 10pt
---

```{r set-options, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(dev = 'pdf',
               comment="", 
               echo=FALSE, warning=TRUE, message=TRUE,
               cache=TRUE, 
               size="footnotesize",
               tidy.opts = list(width.cutoff = 250),
               fig.width = 8, fig.height = 4.5, fig.align = "center")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

color_block = function(color) {
  function(x, options) sprintf('\\color{%s}\\begin{verbatim}%s\\end{verbatim}\\color{black}',
                               color, x)
}
knitr::knit_hooks$set(error = color_block('red'))
knitr::knit_hooks$set(warning = color_block('orange'))
```

```{r libraries, echo=FALSE}
library(tidyverse)
library(magrittr)
library(broom)
```

```{r constants}
chains = 4

options(
  width = 1000,
  mc.cores = min(chains, parallel::detectCores()))
```

```{r functions}
```


# Overview
STATE THE PURPOSE OF THIS DOCUMENT.

# The data for this document

```{r load data, message=F}
d = read_csv("../data/data_ClarkCrowding.csv") %>%
  mutate_at(c("Subject", "Condition"), factor)
d
```


# The linear model (LM)

A linear model (or linear regression) describes an outcome $y$ as the weighted sum of predictors $x_1$, ..., $x_k$, plus some error $\epsilon$ that is assumed to be normally distributed. This can be written in a variety of ways, e.g., for the specific $i$th outcome $y_i$ or for the entire vector of outcomes ${\mathbf y}$ with the elements $y_1$, ..., $y_n$, or in terms of the expected value of $E(\mathbf{y})$:

\begin{align}
y_i & = & \beta_0+\beta_1x_{1,i}+...+\beta_kx_{k,i} + \epsilon_i, & \ \epsilon_i \sim {\rm Normal}(0, \sigma_{resid}) & \Leftrightarrow \\
{\mathbf y} & = &  \beta_0+\beta_1{\mathbf x_1}+...+\beta_k{\mathbf x_k} + \epsilon, & \ \epsilon_i \sim {\rm Normal}(0, \sigma_{resid})  & \Leftrightarrow \\
{\mathbf y} & = & X\beta + \epsilon, & \ \epsilon_i \sim {\rm Normal}(0, \sigma_{resid})  & \Leftrightarrow \\
E({\mathbf y}) & = & X\beta &  &
\end{align}


# Coding categorical (and other) predictors

If we want to include categorical predictors (e.g., "Condition A" vs. "Condition B") in an LM, we thus need to translate these predictors into numerical variables. This process is sometimes called predictor *coding*. Some people use this term narrowly to only refer to the coding of categorical predictors (or, as they are sometimes called, *factors*; hence the name factor coding). Others use it to refer to any changes or transformation we make to our predictors, including continuous predictors (e.g. centering or scaling). In this tutorial, we talk about both.

If you have been using analysis of variance (ANOVA), coding is a process that you might already be familiar with: in ANOVA-based result reporting, we regularly follow the initial ANOVA significance tests with so called "planned" or "post-hoc" comparisons. These comparisons assess hypotheses about the means of the different experimental conditions. 

You might have heard terms like (forward/backward) Helmert coding, (forward/backward) sliding difference coding, polynomial coding, sum/deviation/ANOVA coding, or treatment/dummy coding, etc. These are all names for different ways to code categorical factors, and they do exactly the same job in an LM as they do for the planned or unplanned post-hoc analyses after an ANOVA. In fact, the post-hoc tests reported for ANOVA-based approaches are typically run as an LM, but that's another topic. (Note that these post-hoc tests are the only way to obtain information about the *direction* of an effect in the ANOVA approach, since the ANOVA itself only assesses the significance of a predictor). 

In an LM, we need to make these coding decision *prior* to fitting the model. **Most statistics programs have default coding choices built-in that they apply to any LM or other regression you fit. Be aware of these defaults, as they will affect what the output of an LM means if you forgot to explicitly define the factor coding for the LM.** In R, for example, all factors for which we have not explicitly defined coding will be treatment coded, with the reference level set to the first level of the factor (we'll get to what all of this means). And, if you haven't defined how the levels of the factor order, R will assume that they order alphanumerically. 

Such defaults can be handy in that they mean you can just run a model like: 

$$ Theshold \sim 1 + Condition $$

where Threshold is the outcome, $\sim$ refers to "regress against", 1 refers to the intercept (which by default would be included anyway, but I'm being explicit here), and Condition is a factor with two levels "crowded" and "uncrowded". But defaults can also wreak havoc. In this example, R would automatically code "crowded" as the reference level (0), because it comes alphanumerically first, and "non-crowded" as the treatment (1). The coefficient for Condition would thus---perhaps counter-intuitively---reflect the increase in the threshold in the *un*crowded condition relative to the crowded condition. In short:

 + **don't rely on default**; 
 + **code your factors explicitly**;
 + **use variable names and level names that are transparent and avoid confusion**


# Coding a binary categorical predictor
 
Let's start with the simplest case: coding a binary (two-level) factor. This will allow us to build intuitions both about *how* we can code variables, and *what the consequences of those codes are*. We'll stick with the example from above. 

Factor coding essentially translates the categorical predictor into a numerical predictor. This is achieved through so called *contrasts*. Specifically, for a factor with $k$ distinct levels we can use $k-1$ contrasts to create $k-1$ numerical predictors. These numerical predictors capture all the information of the $k$ levels of the original factor. Each contrast is a vector with $k$ numeric elements, describing the mapping from factor levels to numerical values.

For a binary factor, we thus have one contrast that is a vector with two values. This contrast defines one numerical predictor. E.g., by default in R, we'd get the following for Ashley's data set on visual crowding:

```{r}
contrasts(d$Condition)
```

And, each time a regression function is called, this allows R to *implicilty* create a new column in our data that is the numerical translation of the Condition variable. Thus a model like

$$ Theshold \sim 1 + Condition $$

is actually running the model

$$ Theshold \sim 1 + ConditionUncrowded $$

where $ConditionUncrowded$ is a numerical variable that has been silently added to our data:

```{r}
d %>%
  mutate(ConditionUncrowded = ifelse(Condition == "Uncrowded", 1, 0))
```

## Treatment/dummy-coding

The default in many statistics programs is treatment-coding, like in the example we just went through. Under this coding scheme, we contrast all other condition against a baseline (or "reference") condition. Since the default is in this case somewhat counter-intuitive---it's more intuitive to think of visual *crowding* as the treatment---we set define our own treatment coding.

```{r}
contrasts(d$Condition) = cbind("Crowded" = c(1,0))
contrasts(d$Condition)
l.treatment = lm(Threshold ~ 1 + Condition, data = d)
summary(l.treatment)
```

In this treatment-coded model:

 + the intercept estimate give us the model's prediction for the mean outcome *of the reference level* of Condition (because the intercept *always* gives us the model's prediction when all other terms of the model are 0---e.g., when all other predictors are 0). In this case, this is the mean threshold for the uncrowded condition.
 + the ConditionCrowded estimate tells us how much larger (or smaller) the threshold is for the treatment condition. In this case, this is the crowded condition. 
 + the sum of the intercept and the estimate for the coefficient for ConditionCrowded give us the model's prediction for the mean outcome of the treatment level.
 
Let's visualize that by plotting the model's predictions against the raw data. R has functions for that, but I'm doing it more small-stepped here. First, let's have a look at the model's predictions, and attach. It's just a vector with as many elements as there are outcomes in the data we fit with the linear regression.

```{r}
fitted(l.treatment)
```

Note that the model only predicts two different values---one for the crowded condition and one for the uncrowded condition. This becomes apparent when we attach the fitted/predicted values to the data.frame. Here I'm only showing the Subject, Condition, Threshold, and fitted values:

```{r}
d %<>%
  mutate(fitted = fitted(l.treatment))
d %>% 
  select(Subject, Condition, Threshold, fitted)
```

And visualized:

```{r}
d %>%
  ggplot(aes(x = Condition)) +
  geom_point(aes(y = Threshold, color = Subject), alpha = .5) +
  geom_line(aes(y = Threshold, color = Subject, group = Subject), alpha = .3) +
  stat_summary(aes(y = fitted),
               fun = mean, geom = "point", size = 2) +
  theme_bw()
```

### Question

 + How would you calculate the epsilon/residual for each observation from this data?
 + How could you calculate the residual sum of squares for this model from this data? 
 + Why does the model predict only two values?
 + We obtain the straightforward interpretation of the ConditionCrowded estimate only because we coded the treatment condition as 1, rather than, e.g., as 2. What would happen if we coded the treatment condition as 2? Would our estimate for the coefficient for ConditionCrowded change? What about the estimate of the intercept? Why?

## Deviation/sum/anova-coding

While treatment-coding is the default in the regression world, it is actually rarely used in experimental psychology, brain imaging, or related fields that use factorial/balanced designs. Rather, we typically code our data in a different way, because it changes the interpretation of the intercept in ways that researchers that are used to ANOVA tend to find more intuitive. This alternative is called deviation/sum/anova-coding:

```{r}
contrasts(d$Condition) = cbind("Crowded vs. Uncrowded" = c(.5,-.5))
contrasts(d$Condition)
l.deviation = lm(Threshold ~ 1 + Condition, data = d)
summary(l.deviation)
```

### Questions

 + What happens if we double the values we assign to each of the two deviation-coded conditions? What if we set them to -1000 vs. 1000? What changes and what doesn't? Why?

```{r}
contrasts(d$Condition) = cbind("Crowded vs. Uncrowded" = c(1,-1))
```




# Session info
```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
